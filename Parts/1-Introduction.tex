% Introdução
\chapter{Introduction}

The current volume of data available on the Internet has led to new opportunities and challenges. Since processing huge amounts of data exceeds the capabilities of single machines, the processing of Big Data is often done on a distributed environment. In this scenario the developer has to manage the complexity of data partitioning, distributed computing and a more complex failure handling.

Several research efforts has been done in order to help developers to deal with the complexity of distributed systems, which often take the form of middleware or programming frameworks \cite{ranganathan2007complexity}. One of these initiatives is Apache Spark ~\footnote{https://spark.apache.org/}. It was released in 2010 and has been adopted by many industries, being the most active open source project for big data processing \cite{armbrust2015sparksql}. 

Spark is a cluster computing framework for implementing large-scale data processing applications while providing scalability and fault tolerance \cite{zaharia2010spark}. To use Spark, developers write a program that implements the high-level control flow of their application. The control flow is specified by using one of the supported programming language (Java, Scala, Python, R or SQL) and libraries for extended functionalities. These libraries includes Spark SQL for relational databases, MLib for  machine learning, GraphX for graph processing, as well as a library for stream processing.

Regarding Spark SQL, it extends Spark with a declarative API to allow relational processing, offering features that are essential for an integrated and easy-to-use environment for big Data. According to \cite{armbrust2015sparksql}, user feedback and benchmarks show that Spark SQL makes it significantly simpler and more efficient to write data pipelines that mix relational and procedural processing. 

As stated in \cite{ranganathan2007complexity}, application and service developers often face steep learning curves before they can start programming large distributed systems, since it may have high cognitive complexity if users need to be aware of a large number of concepts in order to use the system. 

To our knowledge, there are no studies that focused on get insights into the issues faced by Spark SQL applications developers. The results of such analysis could improve the state of affairs, being a first step to provide a better toolset with, for instance, practice guidelines, automated test cases generation or static analysis of code.

\section{Motivation}

For the Apache Spark Community~\footnote{https://spark.apache.org/community.html}, the recommended place to find help is the Stack Overflow~\footnote{https://stackoverflow.com/} Q\&A website, as it is an active forum for Spark users’ questions and answers. By consulting Stack Overflow we can state that developers have many problems and questions concerning implementing Spark applications, since there are nearly 40.000 questions tagged with \emph{apache-spark} tag as off June 2018.

In the recent years, several works were conducted  to extract knowledge from Stack Overflow's posts (TODO: Incluir Referências). The analysis of the questions and answers posted on Stack Overflow could reveal interesting insight into the challenges faced
by programmers.

Taxonomy plays an important role in software engineering. The development of a taxonomy of errors in application development is one key step to systematically address and resolve this problem. Such a taxonomy may be developed to categorize the types of errors, to explain why a specific error occurs, and to generate intervention strategies for each type of error.


\section{General Goal}

The main goal of this work is to construct a taxonomy of errors related to the development of Spark SQL applications by extracting knowledge from Stack Overflow in order to help developers.

\section{Specific Goals}

This work aims to address the following three research questions:

\begin{enumerate}
    \item Which are the difficulties that developers often face when implementing Spark SQL applications?
    \item Is the pattern of errors usually faced by Spark SQL developers the same one faced by SQL developers who works with other platforms?
    \item How the API notation changes the pattern of implementation errors when compared to SQL standard statements? 
\end{enumerate}



\section{The Problem Scope}

\section{Contributions}

\section{Thesis Organization}

The remainder of this thesis is organized as follows. In Chapter ...